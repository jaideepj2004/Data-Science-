{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "-> Missing values are the values that are entered as NaN . It is essential to handle the missing value because if we dont handle them then our machine learning model wont properly operate on them . Upsampling , down sampling and smote\n",
    "\n",
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "-> The technique used to handle the missing value by replacing with mean , median or mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0      10.0     100.0\n",
      "1       2.0       NaN     200.0\n",
      "2       NaN      30.0     300.0\n",
      "3       4.0      40.0       NaN\n",
      "4       5.0      50.0     500.0\n",
      "\n",
      "Mean Imputed Dataset:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0      10.0     100.0\n",
      "1       2.0      32.5     200.0\n",
      "2       3.0      30.0     300.0\n",
      "3       4.0      40.0     275.0\n",
      "4       5.0      50.0     500.0\n",
      "\n",
      "Median Imputed Dataset:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0      10.0     100.0\n",
      "1       2.0      35.0     200.0\n",
      "2       3.0      30.0     300.0\n",
      "3       4.0      40.0     250.0\n",
      "4       5.0      50.0     500.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = {\n",
    "    'Feature1': [1, 2, np.nan, 4, 5],\n",
    "    'Feature2': [10, np.nan, 30, 40, 50],\n",
    "    'Feature3': [100, 200, 300, np.nan, 500]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Mean imputation\n",
    "mean_imputed_df = df.fillna(df.mean())\n",
    "\n",
    "# Median imputation\n",
    "median_imputed_df = df.fillna(df.median())\n",
    "\n",
    "# Display the mean imputed dataset\n",
    "print(\"\\nMean Imputed Dataset:\")\n",
    "print(mean_imputed_df)\n",
    "\n",
    "# Display the median imputed dataset\n",
    "print(\"\\nMedian Imputed Dataset:\")\n",
    "print(median_imputed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "  Category  Value\n",
      "0        A   10.0\n",
      "1        B   20.0\n",
      "2        A   15.0\n",
      "3      NaN   25.0\n",
      "4        B   30.0\n",
      "5        A   22.0\n",
      "6        A    NaN\n",
      "7      NaN   18.0\n",
      "8        B   27.0\n",
      "9        B    NaN\n",
      "\n",
      "Mode Imputed Dataset:\n",
      "  Category  Value\n",
      "0        A   10.0\n",
      "1        B   20.0\n",
      "2        A   15.0\n",
      "3        A   25.0\n",
      "4        B   30.0\n",
      "5        A   22.0\n",
      "6        A    NaN\n",
      "7        A   18.0\n",
      "8        B   27.0\n",
      "9        B    NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with missing values in a categorical column\n",
    "data = {\n",
    "    'Category': ['A', 'B', 'A', np.nan, 'B', 'A', 'A', np.nan, 'B', 'B'],\n",
    "    'Value': [10, 20, 15, 25, 30, 22, np.nan, 18, 27, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Mode imputation for the 'Category' column\n",
    "mode_category = df['Category'].mode()[0]  # Calculate the mode\n",
    "df['Category'] = df['Category'].fillna(mode_category)  # Impute missing values with the mode\n",
    "\n",
    "# Display the mode imputed dataset\n",
    "print(\"\\nMode Imputed Dataset:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "-> Imbalanced data is a data where count of a particular type of value or category is much higher than the count of the other category because of this the prediction of our ml model will be mostly the value with higher count.\n",
    "\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n",
    "-> Up-sampling and Down-sampling are techniques that are used to handle an imbalance dataset . We can use Up-sampling and Down-sampling in order to make the count of the different values similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "               Value\n",
      "2022-01-01  0.496714\n",
      "2022-01-02 -0.138264\n",
      "2022-01-03  0.647689\n",
      "2022-01-04  1.523030\n",
      "2022-01-05 -0.234153\n",
      "2022-01-06 -0.234137\n",
      "2022-01-07  1.579213\n",
      "2022-01-08  0.767435\n",
      "2022-01-09 -0.469474\n",
      "2022-01-10  0.542560\n",
      "\n",
      "Upsampled Dataset:\n",
      "                        Value\n",
      "2022-01-01 00:00:00  0.496714\n",
      "2022-01-01 01:00:00  0.470257\n",
      "2022-01-01 02:00:00  0.443799\n",
      "2022-01-01 03:00:00  0.417342\n",
      "2022-01-01 04:00:00  0.390884\n",
      "...                       ...\n",
      "2022-01-09 20:00:00  0.373888\n",
      "2022-01-09 21:00:00  0.416056\n",
      "2022-01-09 22:00:00  0.458224\n",
      "2022-01-09 23:00:00  0.500392\n",
      "2022-01-10 00:00:00  0.542560\n",
      "\n",
      "[217 rows x 1 columns]\n",
      "\n",
      "Downsampled Dataset:\n",
      "               Value\n",
      "2022-01-01  0.335379\n",
      "2022-01-04  0.351580\n",
      "2022-01-07  0.625724\n",
      "2022-01-10  0.542560\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample time series dataset\n",
    "np.random.seed(42)\n",
    "date_rng = pd.date_range(start='2022-01-01', end='2022-01-10', freq='D')\n",
    "data = np.random.randn(len(date_rng))\n",
    "df = pd.DataFrame(data, columns=['Value'], index=date_rng)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Upsample the dataset using linear interpolation\n",
    "upsampled_df = df.resample('H').interpolate(method='linear')\n",
    "\n",
    "# Display the upsampled dataset\n",
    "print(\"\\nUpsampled Dataset:\")\n",
    "print(upsampled_df)\n",
    "\n",
    "# Downsample the dataset by taking the mean of every 3 days\n",
    "downsampled_df = df.resample('3D').mean()\n",
    "\n",
    "# Display the downsampled dataset\n",
    "print(\"\\nDownsampled Dataset:\")\n",
    "print(downsampled_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "-> Data Augmentation is a technique that is used to create synthetic data points , it is a advance version of oversampling . while oversampling follow a pattern that the entire minority data follow to create the new data points , smote constructs new data point between the minority datapoints \n",
    "\n",
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "-> Outlier are value that are either too large or too less , these values dont follow the general pattern that is followed by the data ,and it is essential to handle the outliers because they decrease the accuracy of the model and can cause overfitting .\n",
    "\n",
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "-> I can handle the missing values by either removing the row having missing value , replacing the missing value with either mean , median or mode \n",
    "\n",
    "removing the row having the missing value : good only if we have large data set and only few rows have missing column\n",
    "replacing with mean : good only if the dataset follow a normal distribution \n",
    "replacing with median : good if there are outliers in the dataset\n",
    "replacing with mode : if the value to be replaced is a categorical value \n",
    "\n",
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "->I can plot a pair plot and find the correlation matrix to see if the column having the missing data is correlated to any other column or not \n",
    "I will try to use my common sense to figure how the missing value may be related to any other column \n",
    "\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "-> I can use smote to evaluate the performance of my machine learning model having this imbalance dataset \n",
    "\n",
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "-> I can employ up-sampling , down-sampling or smote in such scenario\n",
    "\n",
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "-> I will use upsampling in this situation as i have to find the occurence of the minority event \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
