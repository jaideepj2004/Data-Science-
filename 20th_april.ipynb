{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "-> KNN stands for K nearest neighbour where k is a hyperparameter , it is a supervised machine learning algorithm and it find the k nearest neighbour for the given input and predicts the output \n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "-> We choose the value of K in KNN by performing GridSearch CV , we also have a hieurestic approach where k = square root of number of datapoints , and if the k value comes out to be even than we either do + 1 or -1 to the k value to make it odd \n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "-> KNN classifier is used to solve the classification problem wheareas KNN regressor is used to solve the regression problem , also KNN classifier finds the classfication by considering the k closest datapoints wheareas KNN regressor find the mean, median , etc of the output of the k nearest input \n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "-> KNN classifier : confusion matrix , r2 square , precision , recall ,etc are used \n",
    "KNN regressor : MAE, MSE , RMSE , R-sqaure , adjusted r square \n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "-> As we increase the dimension of the training dataset , the accuracy of the model reduces this is known as the curse of dimensionality \n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "-> If we have numerical variables then replace the missing value with mean if there is no outliers , replace the missing value with the median if there are missing value and for categorical variables replace the missing value with mode , this is the general method and should be valid for knn as well\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "Performance: Good for non-linear boundaries, not suitable for imbalanced data.\n",
    "Use Cases: Non-linear classification with balanced classes.\n",
    "Strengths: Simple, non-parametric, robust to noise.\n",
    "Weaknesses: Computationally expensive, sensitive to distance metric and k.\n",
    "KNN Regressor:\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "-> KNN Classifier:\n",
    "\n",
    "Performance: Good for non-linear boundaries, not suitable for imbalanced data.\n",
    "Use Cases: Non-linear classification with balanced classes.\n",
    "Strengths: Simple, non-parametric, robust to noise.\n",
    "Weaknesses: Computationally expensive, sensitive to distance metric and k.\n",
    "KNN Regressor:\n",
    "\n",
    "Performance: Effective for non-linear, local relationships.\n",
    "Use Cases: Predicting continuous values with non-linear relationships.\n",
    "Strengths: Simple, non-parametric, handles non-linear relationships.\n",
    "Weaknesses: Computationally expensive, sensitive to distance metric and k.\n",
    "Choose KNN Classifier for non-linear classification with balanced data, and KNN Regressor for predicting continuous values with non-linear relationships.\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "-> Strengths of KNN:\n",
    "\n",
    "Simple implementation.\n",
    "Non-parametric nature.\n",
    "Robust to noisy data.\n",
    "Weaknesses of KNN:\n",
    "\n",
    "Computationally expensive.\n",
    "Sensitive to distance metric and K.\n",
    "Memory intensive.\n",
    "Addressing Weaknesses:\n",
    "\n",
    "Dimensionality reduction.\n",
    "Optimizing K and distance metric.\n",
    "Data preprocessing (normalization/scaling).\n",
    "Use of approximate nearest neighbor methods.\n",
    "Ensemble methods for improved robustness.\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "-> Euclidean Distance measures the shortest straight-line distance between two points in a Euclidean space. In KNN, it calculates the square root of the sum of the squared differences between corresponding coordinates of two points.\n",
    "\n",
    "Manhattan Distance measures the distance between two points in a grid based on the sum of the absolute differences between their coordinates. In KNN, it calculates the sum of the absolute differences between the coordinates of two points.\n",
    "\n",
    "In short, while Euclidean distance measures the straight-line distance, Manhattan distance measures the distance along the grid lines.\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "-> Feature scaling in KNN ensures that all features have similar influence on distance calculation, preventing features with larger scales from dominating the distance metric. This helps improve the accuracy and performance of the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
