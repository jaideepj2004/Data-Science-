{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "-> Min-Max scaling is a scaling technique and it is used to reduce the value of the data point such that they will lie from 0 to 1 , it is commonely used in deep learning where image processing is done \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Feature1  Feature2  Feature3\n",
      "0        10         5         1\n",
      "1        20        15         2\n",
      "2        30        25         3\n",
      "3        40        35         4\n",
      "4        50        45         5\n",
      "\n",
      "Min-Max Scaled Dataset:\n",
      "   Feature1  Feature2  Feature3\n",
      "0      0.00      0.00      0.00\n",
      "1      0.25      0.25      0.25\n",
      "2      0.50      0.50      0.50\n",
      "3      0.75      0.75      0.75\n",
      "4      1.00      1.00      1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50],\n",
    "    'Feature2': [5, 15, 25, 35, 45],\n",
    "    'Feature3': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Create a MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max Scaling to the dataset\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Create a new DataFrame with scaled values\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# Display the Min-Max Scaled dataset\n",
    "print(\"\\nMin-Max Scaled Dataset:\")\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "-> unit vector tecnique reduces the value of a vector such that its magnitude becomes 1 , whereas in Min-Max scaling the value can lie from 0 to 1 . Unit vector can be used for feature scaling in machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Feature1  Feature2  Feature3\n",
      "0         1         2         3\n",
      "1         2         3         4\n",
      "2         3         4         5\n",
      "3         4         5         6\n",
      "4         5         6         7\n",
      "\n",
      "Unit Vector Scaled Dataset:\n",
      "   Feature1  Feature2  Feature3\n",
      "0  0.267261  0.534522  0.801784\n",
      "1  0.371391  0.557086  0.742781\n",
      "2  0.424264  0.565685  0.707107\n",
      "3  0.455842  0.569803  0.683763\n",
      "4  0.476731  0.572078  0.667424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaide\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but Normalizer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [2, 3, 4, 5, 6],\n",
    "    'Feature3': [3, 4, 5, 6, 7]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Create a Normalizer\n",
    "normalizer = Normalizer(norm='l2')  # 'l2' is the default norm for unit vector scaling\n",
    "\n",
    "# Apply unit vector scaling to the dataset\n",
    "scaled_data = normalizer.transform(df)\n",
    "\n",
    "# Create a new DataFrame with scaled values\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# Display the Unit Vector Scaled dataset\n",
    "print(\"\\nUnit Vector Scaled Dataset:\")\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "-> PCA is used to reduce the dimension of the dataset , it is different from feature selection as in feature selection we drop the entire column wheareas in PCA we find the best fit curve having the dimension we want and project all the points on that curve . it is used to reduce the dimension of the dataset \n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "-> PCA helps in reducing the dimension of the dataset by selecting the feature , thus PCA helps in feature extraction \n",
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "->We can apply min max scaler to price , rating and delievery time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Price  Rating  DeliveryTime\n",
      "0     20     4.5             2\n",
      "1     30     3.8             3\n",
      "2     25     4.0             1\n",
      "3     40     4.8             4\n",
      "4     35     4.2             2\n",
      "\n",
      "Min-Max Scaled Dataset:\n",
      "   Price  Rating  DeliveryTime\n",
      "0   0.00     0.7      0.333333\n",
      "1   0.50     0.0      0.666667\n",
      "2   0.25     0.2      0.000000\n",
      "3   1.00     1.0      1.000000\n",
      "4   0.75     0.4      0.333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'Price': [20, 30, 25, 40, 35],\n",
    "    'Rating': [4.5, 3.8, 4.0, 4.8, 4.2],\n",
    "    'DeliveryTime': [2, 3, 1, 4, 2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Create a MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max Scaling to the dataset\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Create a new DataFrame with scaled values\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# Display the Min-Max Scaled dataset\n",
    "print(\"\\nMin-Max Scaled Dataset:\")\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "-> We can use PCA to reduce the dimension by finding the dimension best fit curve as per the advise of the domain expert , and then we will find the best fit curve having that dimension , then we will find the projection of all the points on that curve \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Values: [ 1  5 10 15 20]\n",
      "Scaled Values (Range -1 to 1): [-1. -1.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "values = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate the median\n",
    "median_value = np.median(values)\n",
    "max_value = np.max(values)\n",
    "\n",
    "# Perform Min-Max scaling using the median\n",
    "scaled_values = (values - median_value) //(max_value-median_value)\n",
    "\n",
    "\n",
    "\n",
    "# Display the original and scaled values\n",
    "print(\"Original Values:\", values)\n",
    "print(\"Scaled Values (Range -1 to 1):\", scaled_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [8.30042520e-01 1.50819365e-01 1.91381147e-02 3.78085593e-35]\n",
      "Cumulative Explained Variance: [0.83004252 0.98086189 1.         1.        ]\n",
      "Number of Components to Retain (95% variance): 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([\n",
    "    [170, 65, 30, 0, 120],\n",
    "    [165, 70, 35, 1, 130],\n",
    "    [180, 75, 40, 0, 140],\n",
    "    [160, 60, 25, 1, 110],\n",
    "    [175, 80, 45, 0, 150]\n",
    "])\n",
    "\n",
    "# Separate features and labels\n",
    "features = data[:, :4]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(features_standardized)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of components to retain (e.g., 95% of the variance)\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "# Display results\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_explained_variance)\n",
    "print(\"Number of Components to Retain (95% variance):\", num_components_to_retain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
