{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "-> Gradient Boosting Regression is a supervised machine learning alogrithm which is present in ensemble techniques, more specifically boosting technique , it is used for regression \n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.7269\n",
      "R-squared: 0.9257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "\n",
    "# Generate a small dataset for demonstration\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the gradient boosting model using Scikit-Learn\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best Model Mean Squared Error: 2.6602\n",
      "Best Model R-squared: 0.9275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "# Display results\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Model Mean Squared Error: {mse_best:.4f}\")\n",
    "print(f\"Best Model R-squared: {r2_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "-> The weak learner in a Gradient Boosting is a decision tree and not a stump \n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "-> \n",
    "1. calculate the median of the output \n",
    "2. calculate the residual and train a model to predict that residual \n",
    "3. calculate the output of this model by using the formula Y = mean + learning_rate * model_1\n",
    "5. using this predicted value again calculate the residual \n",
    "6. repeat the above steps until the number of models is equal to the numbe of estimators\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "-> Here sequentially the weak learners are trained and output of the present model is dependent on the ouput of the previous model , more specifically output of the present model depends upon the accuracy of the previous model , this interdepdency and usage of multiple models causes ensemble of weak learners for Gradient Boosting \n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "-> Decision Trees:\n",
    "\n",
    "Use simple decision trees as building blocks.\n",
    "Trees correct errors made by previous trees.\n",
    "Residuals:\n",
    "\n",
    "Start with an initial prediction.\n",
    "Train new models to fix prediction errors (residuals).\n",
    "Boosting:\n",
    "\n",
    "Train models sequentially.\n",
    "Each model corrects errors of the previous ones.\n",
    "Final prediction is a weighted sum of all models.\n",
    "Loss Function:\n",
    "\n",
    "Minimize a problem-specific loss (e.g., mean squared error).\n",
    "Learning Rate:\n",
    "\n",
    "Introduce a learning rate to control each model's impact.\n",
    "Regularization:\n",
    "\n",
    "Use constraints (e.g., tree depth) to prevent overfitting.\n",
    "Stop training when performance stops improving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
