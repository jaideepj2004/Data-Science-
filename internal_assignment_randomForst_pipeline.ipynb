{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict total bill using the tips dataset \n",
    "create pipeline\n",
    "evaluate all possible models before and tune the model that gives the best accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_bill   tip     sex smoker  day    time  size\n",
      "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
      "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
      "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
      "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
      "4       24.59  3.61  Female     No  Sun  Dinner     4\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Load the 'tips' dataset from Seaborn\n",
    "tips_df = sns.load_dataset('tips')\n",
    "\n",
    "# Display all the columns\n",
    "print(tips_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the categorical columns\n",
    "categorical_cols = ['smoker', 'sex', 'time', 'day']\n",
    "\n",
    "# Extract the specified categorical columns from the DataFrame\n",
    "selected_categorical_cols = tips_df[categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['size', 'tip']\n",
    "\n",
    "# Extract the specified numerical columns from the DataFrame\n",
    "selected_num_cols = tips_df[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n",
    "      # Replace outliers with median\n",
    "    ('scaler', StandardScaler())  # Optionally, scale the features (you can remove this step if not needed)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a pipeline for categorical columns\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with mode\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Perform one-hot encoding\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tips_df.drop('total_bill', axis=1)  # Features (excluding the 'total_bill' column)\n",
    "y = tips_df['total_bill']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, num_cols),\n",
    "        ('cat', categorical_pipeline, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([ 7.25,  7.51,  8.51,  8.52,  8.58,  9.6 ,  9.78, 10.07, 10.09,\n       10.29, 10.33, 10.63, 11.02, 11.17, 11.35, 11.59, 11.61, 12.16,\n       12.43, 12.46, 12.54, 12.6 , 12.66, 12.69, 12.74, 12.9 , 13.  ,\n       13.03, 13.13, 13.27, 13.28, 13.37, 13.39, 13.42, 13.51, 13.81,\n       13.94, 14.  , 14.07, 14.26, 14.73, 14.83, 15.01, 15.04, 15.06,\n       15.36, 15.42, 15.48, 15.53, 15.69, 15.77, 15.81, 15.95, 15.98,\n       16.  , 16.21, 16.27, 16.29, 16.31, 16.32, 16.4 , 16.47, 16.58,\n       16.66, 16.82, 16.99, 17.07, 17.29, 17.31, 17.46, 17.47, 17.59,\n       17.78, 17.82, 17.92, 18.04, 18.15, 18.24, 18.28, 18.29, 18.35,\n       18.43, 18.78, 19.08, 19.44, 19.49, 19.77, 19.81, 20.08, 20.27,\n       20.29, 20.45, 20.49, 20.53, 20.76, 21.01, 21.16, 21.7 , 22.12,\n       22.23, 22.42, 22.49, 22.75, 22.76, 23.1 , 23.17, 23.33, 24.01,\n       24.06, 24.27, 24.52, 24.59, 24.71, 25.  , 25.21, 25.29, 25.56,\n       25.71, 26.41, 26.86, 27.2 , 27.28, 28.15, 28.17, 28.55, 29.03,\n       29.85, 29.93, 30.14, 30.4 , 30.46, 31.27, 31.71, 31.85, 32.4 ,\n       32.68, 32.83, 34.3 , 34.63, 34.65, 34.81, 34.83, 35.26, 38.01,\n       38.73, 39.42, 40.17, 40.55, 44.3 , 45.35, 48.27, 50.81]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Use the function to get mean squared error scores\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m mse_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_regressors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregressors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_split\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Display mean squared error results\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Squared Error Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mse_results)\n",
      "Cell \u001b[1;32mIn[46], line 36\u001b[0m, in \u001b[0;36mevaluate_regressors\u001b[1;34m(regressors, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     33\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, regressor \u001b[38;5;129;01min\u001b[39;00m regressors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Train the regressor\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:263\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_refit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:422\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _refit:\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[43m_check_partial_fit_first_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39mfirst_call)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:422\u001b[0m, in \u001b[0;36m_check_partial_fit_first_call\u001b[1;34m(clf, classes)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`classes=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m` is not the same as on last call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto partial_fit, was: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (classes, clf\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;66;03m# This is the first call to partial_fit\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m         clf\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# classes is None and clf.classes_ has already previously been set:\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# nothing to do\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:105\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    103\u001b[0m _unique_labels \u001b[38;5;241m=\u001b[39m _FN_UNIQUE_LABELS\u001b[38;5;241m.\u001b[39mget(label_type, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _unique_labels:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mrepr\u001b[39m(ys))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_array_api_compliant:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# array_api does not allow for mixed dtypes\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     unique_ys \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mconcat([_unique_labels(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys])\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: (array([ 7.25,  7.51,  8.51,  8.52,  8.58,  9.6 ,  9.78, 10.07, 10.09,\n       10.29, 10.33, 10.63, 11.02, 11.17, 11.35, 11.59, 11.61, 12.16,\n       12.43, 12.46, 12.54, 12.6 , 12.66, 12.69, 12.74, 12.9 , 13.  ,\n       13.03, 13.13, 13.27, 13.28, 13.37, 13.39, 13.42, 13.51, 13.81,\n       13.94, 14.  , 14.07, 14.26, 14.73, 14.83, 15.01, 15.04, 15.06,\n       15.36, 15.42, 15.48, 15.53, 15.69, 15.77, 15.81, 15.95, 15.98,\n       16.  , 16.21, 16.27, 16.29, 16.31, 16.32, 16.4 , 16.47, 16.58,\n       16.66, 16.82, 16.99, 17.07, 17.29, 17.31, 17.46, 17.47, 17.59,\n       17.78, 17.82, 17.92, 18.04, 18.15, 18.24, 18.28, 18.29, 18.35,\n       18.43, 18.78, 19.08, 19.44, 19.49, 19.77, 19.81, 20.08, 20.27,\n       20.29, 20.45, 20.49, 20.53, 20.76, 21.01, 21.16, 21.7 , 22.12,\n       22.23, 22.42, 22.49, 22.75, 22.76, 23.1 , 23.17, 23.33, 24.01,\n       24.06, 24.27, 24.52, 24.59, 24.71, 25.  , 25.21, 25.29, 25.56,\n       25.71, 26.41, 26.86, 27.2 , 27.28, 28.15, 28.17, 28.55, 29.03,\n       29.85, 29.93, 30.14, 30.4 , 30.46, 31.27, 31.71, 31.85, 32.4 ,\n       32.68, 32.83, 34.3 , 34.63, 34.65, 34.81, 34.83, 35.26, 38.01,\n       38.73, 39.42, 40.17, 40.55, 44.3 , 45.35, 48.27, 50.81]),)"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize regressors\n",
    "svm_regressor = SVR()\n",
    "lr_regressor = LinearRegression()\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "nb_regressor = GaussianNB()  # Using Gaussian Naive Bayes for regression\n",
    "\n",
    "# Store regressors in a dictionary\n",
    "regressors = {\n",
    "    'SVR': svm_regressor,\n",
    "    'Linear Regression': lr_regressor,\n",
    "    'Random Forest': rf_regressor,\n",
    "    'Decision Tree': dt_regressor,\n",
    "    'KNN': knn_regressor,\n",
    "    'Naive Bayes': nb_regressor\n",
    "}\n",
    "\n",
    "# Define a function to train and evaluate regressors\n",
    "def evaluate_regressors(regressors, X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    for name, regressor in regressors.items():\n",
    "        # Train the regressor\n",
    "        regressor.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = regressor.predict(X_test)\n",
    "        \n",
    "        # Evaluate performance using mean squared error\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        # Store results in the dictionary\n",
    "        results[name] = mse\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Use the function to get mean squared error scores\n",
    "mse_results = evaluate_regressors(regressors, X_train_split, X_test_split, y_train_split, y_test_split)\n",
    "\n",
    "# Display mean squared error results\n",
    "print(\"Mean Squared Error Results:\", mse_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for SVR:\n",
      "Mean Squared Error: 46.5527\n",
      "R-squared: 0.4510\n",
      "\n",
      "Metrics for Linear Regression:\n",
      "Mean Squared Error: 31.8736\n",
      "R-squared: 0.6241\n",
      "\n",
      "Metrics for Random Forest:\n",
      "Mean Squared Error: 42.9438\n",
      "R-squared: 0.4935\n",
      "\n",
      "Metrics for Decision Tree:\n",
      "Mean Squared Error: 41.1227\n",
      "R-squared: 0.5150\n",
      "\n",
      "Metrics for KNN:\n",
      "Mean Squared Error: 54.0607\n",
      "R-squared: 0.3624\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Assume 'total_bill' is your target variable\n",
    "X = tips_df.drop(\"total_bill\", axis=1)\n",
    "y = tips_df[\"total_bill\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor\n",
    "numeric_features = ['size', 'tip']\n",
    "categorical_features = ['smoker', 'sex', 'time', 'day']\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder())]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize regressors (excluding Naive Bayes)\n",
    "svm_regressor = SVR()\n",
    "lr_regressor = LinearRegression()\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "\n",
    "# Store regressors in a dictionary\n",
    "regressors = {\n",
    "    'SVR': svm_regressor,\n",
    "    'Linear Regression': lr_regressor,\n",
    "    'Random Forest': rf_regressor,\n",
    "    'Decision Tree': dt_regressor,\n",
    "    'KNN': knn_regressor,\n",
    "}\n",
    "\n",
    "# Function to evaluate regressors\n",
    "def evaluate_regressors(regressors, X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    for name, regressor in regressors.items():\n",
    "        regressor_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                              ('regressor', regressor)])\n",
    "        regressor_pipeline.fit(X_train, y_train)\n",
    "        y_pred = regressor_pipeline.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results[name] = {'Mean Squared Error': mse, 'R-squared': r2}\n",
    "    return results\n",
    "\n",
    "# Use the function to get evaluation results\n",
    "evaluation_results = evaluate_regressors(regressors, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display evaluation results\n",
    "for name, metrics in evaluation_results.items():\n",
    "    print(f\"\\nMetrics for {name}:\")\n",
    "    print(f\"Mean Squared Error: {metrics['Mean Squared Error']:.4f}\")\n",
    "    print(f\"R-squared: {metrics['R-squared']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'regressor__copy_X': True, 'regressor__fit_intercept': True, 'regressor__n_jobs': None, 'regressor__positive': False}\n",
      "Best Negative Mean Squared Error: -36.1832853191179\n",
      "\n",
      "Test Set Metrics for Best Linear Regression Model:\n",
      "Mean Squared Error: 31.8736\n",
      "R-squared: 0.6241\n"
     ]
    }
   ],
   "source": [
    "lr_regressor = LinearRegression()\n",
    "\n",
    "# Pipeline\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lr_regressor)])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {\n",
    "    'regressor__copy_X': [True, False],\n",
    "    'regressor__fit_intercept': [True, False],\n",
    "    'regressor__n_jobs': [None, 1, 2, 3],  # Replace None with the desired value\n",
    "    'regressor__positive': [False, True]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(lr_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and corresponding mean cross-validated score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Negative Mean Squared Error:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "y_pred = best_lr_model.predict(X_test)\n",
    "\n",
    "# Print the Mean Squared Error and R-squared on the test set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"\\nTest Set Metrics for Best Linear Regression Model:\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
