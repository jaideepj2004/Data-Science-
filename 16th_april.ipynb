{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?\n",
    "\n",
    "-> Boosting is a technique used in supervised machine learning , here we sequentially train the weak learners and eventually end up making a better model \n",
    "\n",
    "Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "-> Advantaages :\n",
    "1. higher accuracy in prediction \n",
    "2. low bias and low variance \n",
    "3. can train on any dataset , unlike linear regression which does not perform well in a non linear data \n",
    "Disadvantages : \n",
    "1. required high computational power\n",
    "2. relatively slow \n",
    "3. may cause overfitting \n",
    "\n",
    "Q3. Explain how boosting works.\n",
    "\n",
    "-> In boosting there are broadly two method , the one which is used in ada booost and the second type of method is used for gradient boost and xgboost \n",
    "\n",
    "1. all the weaklearners are trained and the drawbacks/errors of the present model is highlighted in the next model , and this sequential pattern continuous \n",
    "\n",
    "2. In second type the first weak learner is just takes the average of the output , and than residual is calculated which is than predicted by the next model and again using the prediction of this model , learning rate and the average , final prediction is calculated and than again the residual is calculated and this process continous \n",
    "\n",
    "there are different methods to calculate the residuals for the classification and regression \n",
    "\n",
    "Q4. What are the different types of boosting algorithms?\n",
    "\n",
    "-> There are three types of boosting algorithm \n",
    "\n",
    "1. Adaboost \n",
    "2. Gradient boost \n",
    "3. Xgboost\n",
    "\n",
    "Q5. What are some common parameters in boosting algorithms?\n",
    "\n",
    "-> Some of the common parameters in the boosting algorithm are : \n",
    "1. Estimators \n",
    "2. Max depth of the tree \n",
    "3. split_check \n",
    "\n",
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "-> In boosting algorithm , the present model drawbacks are highlighted for the next model and the next model focus on the error made by the previous model , in this way the sequence continuous and thus finally the stronger learner is created by the sequential joint of this weak learners\n",
    "\n",
    "Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "-> In ada boost we first train the present model , highlight it shortcoming by upsampling the outputs where the present model gave wrong output and this new dataset is sent for the training of the next model , here we use stumps and not trees\n",
    "\n",
    "below is the detailed exaplanation for this : (classification)\n",
    "\n",
    "1. Calculate the models prediction \n",
    "2. find the points where there is error \n",
    "3. assign weights to every row which is initally equal (1/n) where n is the number of rows present in a dataset\n",
    "4. calculate error which is sum of all the weights wherever there is wrong prediction \n",
    "5. calculate the new weights \n",
    "6. calculate the range\n",
    "7. randomly generate datapoints\n",
    "8. feed the upsampled dataset to the next model \n",
    "\n",
    "Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "-> y = aplha_1 * m1 + alpha_2*m2 ... alpha_n*mn \n",
    "\n",
    "where alpha is the weight and mi is the prediction given by the ith model \n",
    "\n",
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "-> it uses the formula previous weight * exp(-alpha) \n",
    "\n",
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "-> It may cause overfitting\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
